name: dipo
version: denoiser

data:
  name: dm_dipo
  json_root: data_path
  root: data_path   # root directory of the dataset
  batch_size: 20  # batch size for training
  num_workers: 8  # number of workers for data loading
  K: 32    # maximum number of nodes (parts) in the graph (object)
  split_file: split_file_path
  n_views_per_model: 20
  frame_mode: last_frame
  test_which: pm
  mode_num: 5

system:
  name: sys_origin
  exp_dir: ./exps/${name}/${version}
  data_root: ${data.root}
  n_time_samples: 16
  loss_fg_weight: 0.01
  img_drop_prob: 0.1    # image dropout probability, for classifier free training
  guidance_scaler: 0.5  # scaling factor for guidance on the image during inference
  graph_drop_prob: 0.5  # graph dropout probability, for classifier free training

  model:
    name: denoiser
    in_ch: 6
    attn_dim: 128
    n_head: 4
    n_layers: 6
    dropout: 0.1
    K: ${data.K}
    mode_num: 5
    img_emb_dims: [768, 128]
    cat_drop_prob: 0.5      # object category dropout probability, for classifier free training

  scheduler:  # scheduler for the diffusion model
    name: ddpm
    config:
      num_train_timesteps: 1000
      beta_schedule: linear
      prediction_type: epsilon

  lr_scheduler_adapter: # lr scheduler for the new modules on top of the base model
    name: LinearWarmupCosineAnnealingLR
    warmup_epochs: 3
    max_epochs: ${trainer.max_epochs}
    warmup_start_lr: 1e-6
    eta_min: 1e-5

  optimizer_adapter: # optimizer for the new modules on top of the base model
    name: AdamW
    args:
      lr: 5e-4
      betas: [0.9, 0.99]
      eps: 1.e-15

  lr_scheduler_cage: # lr scheduler for modules in the base model
    name: LinearWarmupCosineAnnealingLR
    warmup_epochs: 3
    max_epochs: ${trainer.max_epochs}
    warmup_start_lr: 1e-6
    eta_min: 1e-5

  optimizer_cage: # optimizer for modules in the base model
    name: AdamW
    args:
      lr: 5e-5
      betas: [0.9, 0.99]
      eps: 1.e-15

checkpoint:
  dirpath: ${system.exp_dir}/ckpts
  save_top_k: -1
  every_n_epochs: 50

logger: # wandb logger
  save_dir: ${system.exp_dir}/logs # directory to save logs
  name: ${name}_${version}
  project: SINGAPO

trainer:
  max_epochs: 200
  log_every_n_steps: 100
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  check_val_every_n_epoch: 10
  precision: 16-mixed
  profiler: simple
  num_sanity_val_steps: -1

